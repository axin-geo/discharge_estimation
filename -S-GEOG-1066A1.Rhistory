stat_smooth(aes(dy$dQ_R, dy$dV_et_R), method = "lm", col = "red", se = FALSE)
dV_et_R <- data.frame(datetime = y$datetime, dV_et_R = dy$dV_et_R) %>% xts(., order.by = .$datetime) %>% subset(., select = -datetime)
mx <- matrix(nrow = nrow(dy))
for (i in 1:R) {
## adding R-1 rows of NA of sequence
r <- rbind(seq(from = i, to = nrow(y), by = R), matrix(ncol = length(seq(from = i, to = nrow(y), by = R)), nrow = R-1))
## adding NA before the first value from the vector
r_NA <- c(rep(NA,i-1), r)
## length correction
length(r_NA) <- nrow(y)
mx <- as.data.frame(cbind(mx, r_NA)); names(mx)[i+1] <- paste0(c("dQ"), i)
}
# drop defaut column from df
mx <- subset(mx, select = -V1) # View(mx)
# convert mx to time series object
mx <- zoo(mx, dy$datetime)
dy <- dy  %>% xts(., order.by = .$datetime) %>% subset(., select = -c(datetime,ET,dV_et_R))
# add number of days as a new column
day <- seq(from = 1, to = nrow(dy)); dy <- cbind(dy, day)
# join indices of sampling days (mx) to dy
dy <- merge(dy, mx)
# Due to sampling gaps, dQ is remotely measured every R days by SWOT.
# Here we simulate the effects of sampling gaps and assume dQ on days without measurements takes on a linear distribution btw sampling days.
# add dQ to sampling days
for (i in 1:R){
for (j in 1:nrow(dy)){
if (!is.na(dy[j, i+5]))
dy[j, i+5] <- dy[j, 1]
}}
# Interpolate the discharge differences dQ on days without measurements (linear)
# Assume the daily volumes of discharge take on a linear variation
dy <- na_interpolation(dy, option = "linear") %>% subset(., select = -c(day,dQ,dV))
# Rescale/Aggregate daily dQ and dV by a time period of R (representing SWOT time scale)
# dQ in a unit of millin m3/day
for (i in 1:R){
for (j in 1:((nrow(dy)-i)%/%R)){
a <- R*(j - 1) + i + 1; b <- j*R + i
dy[j*R + i, i + 2] <- sum(dy[,i+2][a:b])
dy[,i+2][a:(b-1)] <- NA
}
}
# Code from Aote Xin, up to June 8, 2020
# SWOT reservoir-river mass balance
# [NOTES: THESE CODES WERE DEVELOPED FOR 'TESTING A PROPOSED ALGORITHM FOR ESTIMATING WATER DISCHARGE AT RIVER-RESERVOIR INTERFACE: POTENTIAL APPLICATIONS FOR THE SURFACE WATER AND OCEAN TOPOGRAPHY SATELLITE MISSION' PROJECT]
# installing and loading packages that are used in the following script
# install.packages(c("dplyer", "tidyverse","ggplot2", "xts","dygraphs","imputeTS"))
library("dplyr");library("rio");library("tidyverse");library("readxl");library("xts");library("dygraphs");library("imputeTS");library("ggplot2");library("zoo")
# Read and convert raw data
# Here we will read the long-term monthly evaportranspiration data developed by Dr. Gao Huilin for Possum Kingdom lake
s_name <- "Possum Kingdom Lk_09_10"
R <- 11 # sampling gap / temporal resolution
et_GRAND_ID <- 1176; et <- read.table(paste0("C:\\Users\\axin\\OneDrive - Kansas State University\\SWOT_from_Aote\\Supporting data\\ET\\Reservoir_evaporation721\\", et_GRAND_ID, ".txt"), header = T, stringsAsFactors = FALSE)
start_mon <- which(et$Month == 20081001, arr.ind = TRUE); end_mon <- which(et$Month == 20100901, arr.ind = TRUE)
start_yr <- 2008; end_yr <- 2010
head(et, 6)
# In the next chunk, the in situ data are read which includes the discharge measurements for inflow and outflow rivers and reservoir storage measurements all at a daily basis. Data acquired from USGS NWIS.
y <- excel_sheets(paste0("C:/Users/axin/OneDrive - Kansas State University/SWOT_from_Aote/raw_data_by_num_of_streams/out_1_in_1/", s_name, ".xlsx")) %>%
map(~read_xlsx(paste0("C:/Users/axin/OneDrive - Kansas State University/SWOT_from_Aote/raw_data_by_num_of_streams/out_1_in_1/", s_name, ".xlsx"),.)) %>% ## convert multiple sheets from Excel into one sheet
data.frame()
# tidy data
n = c("V", "Q_out", "Q_in"); m = c("site_V", "site_out", "site_in")
# function "tidy" extracts useful columns from the raw data frame and returns a single dataframe with date, site numbers, discharge measurements for inflow and outflow rivers and reservoir storage measurements
# 32400: observation at midnight; 00003: daily mean observation; 30600: observation at 6:00am; 30800: observation at 8:00am
tidy <- function(s){
s <- s %>% select(., datetime, contains("site_no"), ends_with(c("32400","30800","30600")), contains("00003"),-contains("cd")) %>%
filter(., site_no != "15s") %>% rename_at(vars(c(5,6,7)), ~ n) %>% rename_at(vars(contains("site")), ~ m)
s$datetime <- as.Date(as.numeric(s$datetime), origin = "1899-12-30")
return(s)}
y <- tidy(y); head(y, 6)
# Derive and plot daily dV and dQ
# Unit Conversion
convert_af_mcm = 1233.48/10.0^6 ## convert from Thousand acre feet to million m3
convert_cfs_mcmd = 3600.0*24.0*0.0283168/10.0^6 ## convert from cubic feet per second to million m3 per day;
y$V <- as.numeric(y$V) * convert_af_mcm;
y$Q_out <- as.numeric(y$Q_out) * convert_cfs_mcmd;
y$Q_in <- as.numeric(y$Q_in) * convert_cfs_mcmd;
# calculate mass balance
dV <- y$V %>% append(.,NA, 0); length(dV) <- nrow(y); dV <- y$V - dV
y <- y %>% mutate(.,dQ = Q_in - Q_out) %>% cbind(., dV)
# correlation of dV and dQ
cor(y$dV, y$dQ, use = "complete.obs")
# Plot dV vs dQ
range_limit <- max(abs(min(min(y$dV, na.rm = TRUE), min(y$dQ,na.rm = TRUE))),
abs(max(max(y$dV,na.rm = TRUE), max(y$dQ, na.rm = TRUE))))
ggplot(y) +
geom_point(mapping = aes(x = dQ, y = dV), color="darkgreen", shape= 17, alpha = 1/2, size = 3, na.rm = TRUE) +
labs(title ="Daily dQ vs dV", x = "dQ (m^3)", y = "dV (m^3)") +
xlim(-range_limit, range_limit) +  ylim(-range_limit, range_limit) +
geom_segment(aes(x = min(min(y$dV, na.rm = TRUE), min(y$dQ,na.rm = TRUE)),  y = min(min(y$dV, na.rm = TRUE), min(y$dQ,na.rm = TRUE)), xend = max(max(y$dV,na.rm = TRUE), max(y$dQ, na.rm = TRUE)), yend = max(max(y$dV,na.rm = TRUE), max(y$dQ, na.rm = TRUE))),
linetype = "dashed")
# Rescale: ET data
# This code chunk rescales the monthly average ET data to a time scale of daily.
head(et, 6)
# current unit million m3/month to million m3/day
# Note: different month has different num of days
et_mon <- et[start_mon:end_mon,]
# Unit Conversion for ET
convert_tcm_mcm = 0.001 ## convert from Thousand m3/month to million m3/month
et_mon$ET <- apply(et_mon[6:8],1 , mean)
et_mon <- et_mon %>%  select(., -c(2:8)) %>% mutate(., ET = ET*convert_tcm_mcm)
et_mon$Month <- as.Date(strptime(et_mon$Month, "%Y%m%d"))
et_day <- left_join(y, et_mon, by = c("datetime" = "Month")) %>% select(c("datetime","ET"))
et_day <- separate(et_day, 1, c("yr", "mon", "day"), convert = T)
for (i in start_yr:end_yr){
for (j in min(et_day[et_day$yr == i,]$mon):max(et_day[et_day$yr == i,]$mon)){
ET_daily <- et_day[et_day$yr== i & et_day$mon == j,][1,4]/nrow(et_day[et_day$yr== i & et_day$mon == j,])
for (k in 1:nrow(et_day[et_day$yr== i & et_day$mon == j,])){
et_day[et_day$yr== i & et_day$mon == j,][k,4] <- ET_daily
}
}
}
et_day <- et_day %>% unite("datetime", c("yr","mon", "day"), sep = "-"); et_day$datetime <- as.Date(et_day$datetime)
head(et_day, 6)
# Rescale: in situ data
# This code chunk rescales daily dQ, dV and dV added up with daily ET derived from previous codes by a time period of R (representing SWOT time scale)
# Using daily V and V_et rather than daily dV to calculate rescaled dV and V_et(in case some daily dV has missing in situ observations)
# Using dQ to calculate rescaled dQ
# Note: dQ in a unit of millin m3/day, dV in a unit of million m3
dy <- y %>% select(., datetime, dQ, dV, V) %>% left_join(., et_day) %>% mutate(., dQ_R = NA,V_R = NA, dV_R = NA, V_et = NA, V_et_R = NA, dV_et_R = NA)
# dQ_R
i <- 1;while (i * R < nrow(dy)){dy$dQ_R[R*i+1] <- sum(dy$dQ[(2+i*R-R):(i*R+1)], na.rm = T); i <- i+1}
# V_R
dy$V_R[1] <- dy$V[1];for (i in 2:(nrow(dy)-1)){
if(i %% R == 0) dy$V_R[i+1] <- dy$V[i+1]}
# dV_R
i <- 1;
while (i*R < nrow(dy)){dy$dV_R[i*R + 1] <- (dy$V_R[i*R +1] - dy$V_R[(i-1)*R + 1]); i <- i + 1}
# V_et
cumu_et <- data.frame(cumu = rep(NA, nrow(dy)));for (i in 1:nrow(dy)){  cumu_et[i,] <- sum(dy$ET[1:i], na.rm = T) }
dy$V_et <- cumu_et$cumu + dy$V
# V_et_R
dy$V_et_R[1] <- dy$V_et[1]
for (i in 2:(nrow(dy)-1)){if(i %% R == 0) dy$V_et_R[i+1] <- dy$V_et[i+1]}
# dV_et_R
i <- 1;while (i*R < nrow(dy)){dy$dV_et_R[i*R + 1] <- (dy$V_et_R[i*R +1] - dy$V_et_R[(i-1)*R + 1]); i <- i + 1}
dy <- dy %>%
select(datetime, dQ, dV, dV_R, dQ_R, dV_et_R)
head(dy,20)
head(y)
# Plotting dQ_R vs dV_R & dV_et_R
range_limit <- max(abs(min(min(dy$dV_R, na.rm = TRUE), min(dy$dQ_R, na.rm = TRUE))), abs(max(max(dy$dV_R, na.rm = TRUE), max(dy$dQ_R, na.rm = TRUE))))
ggplot(dy) +
geom_point(mapping = aes(x = dQ_R, y = dV_R), color="darkgreen", shape= 17, alpha = 1/2, size = 2, na.rm = TRUE) +
geom_segment(aes(x = min(min(dy$dV_R, na.rm = TRUE), min(dy$dQ_R, na.rm = TRUE)), y = min(min(dy$dV_R, na.rm = TRUE), min(dy$dQ_R, na.rm = TRUE)), xend = max(max(dy$dV_R, na.rm = TRUE), max(dy$dQ_R, na.rm = TRUE)), yend = max(max(dy$dV_R, na.rm = TRUE), max(dy$dQ_R, na.rm = TRUE))),linetype = "dashed") +
geom_point(mapping = aes(x = dQ_R, y = dV_et_R), color="red", shape= 17, alpha = 1/2, size = 2, na.rm = TRUE) +
labs(title =paste0("Storage-discharge balance for \n", s_name, " \n(Sampling Gap: ", R, " days)"), x = "dQ (million m^3)", y = "dV (million m^3)") +
xlim(-range_limit, range_limit) + ylim(-range_limit, range_limit) +
stat_smooth(aes(dy$dQ_R, dy$dV_et_R), method = "lm", col = "red", se = FALSE)
dV_et_R <- data.frame(datetime = y$datetime, dV_et_R = dy$dV_et_R) %>% xts(., order.by = .$datetime) %>% subset(., select = -datetime)
# Code from Aote Xin, up to June 8, 2020
# SWOT reservoir-river mass balance
# [NOTES: THESE CODES WERE DEVELOPED FOR 'TESTING A PROPOSED ALGORITHM FOR ESTIMATING WATER DISCHARGE AT RIVER-RESERVOIR INTERFACE: POTENTIAL APPLICATIONS FOR THE SURFACE WATER AND OCEAN TOPOGRAPHY SATELLITE MISSION' PROJECT]
# installing and loading packages that are used in the following script
# install.packages(c("dplyer", "tidyverse","ggplot2", "xts","dygraphs","imputeTS"))
library("dplyr");library("rio");library("tidyverse");library("readxl");library("xts");library("dygraphs");library("imputeTS");library("ggplot2");library("zoo")
# Read and convert raw data
# Here we will read the long-term monthly evaportranspiration data developed by Dr. Gao Huilin for Possum Kingdom lake
s_name <- "Possum Kingdom Lk_09_10"
R <- 11 # sampling gap / temporal resolution
et_GRAND_ID <- 1176; et <- read.table(paste0("C:\\Users\\axin\\OneDrive - Kansas State University\\SWOT_from_Aote\\Supporting data\\ET\\Reservoir_evaporation721\\", et_GRAND_ID, ".txt"), header = T, stringsAsFactors = FALSE)
start_mon <- which(et$Month == 20081001, arr.ind = TRUE); end_mon <- which(et$Month == 20100901, arr.ind = TRUE)
start_yr <- 2008; end_yr <- 2010
head(et, 6)
# In the next chunk, the in situ data are read which includes the discharge measurements for inflow and outflow rivers and reservoir storage measurements all at a daily basis. Data acquired from USGS NWIS.
y <- excel_sheets(paste0("C:/Users/axin/OneDrive - Kansas State University/SWOT_from_Aote/raw_data_by_num_of_streams/out_1_in_1/", s_name, ".xlsx")) %>%
map(~read_xlsx(paste0("C:/Users/axin/OneDrive - Kansas State University/SWOT_from_Aote/raw_data_by_num_of_streams/out_1_in_1/", s_name, ".xlsx"),.)) %>% ## convert multiple sheets from Excel into one sheet
data.frame()
# tidy data
n = c("V", "Q_out", "Q_in"); m = c("site_V", "site_out", "site_in")
# function "tidy" extracts useful columns from the raw data frame and returns a single dataframe with date, site numbers, discharge measurements for inflow and outflow rivers and reservoir storage measurements
# 32400: observation at midnight; 00003: daily mean observation; 30600: observation at 6:00am; 30800: observation at 8:00am
tidy <- function(s){
s <- s %>% select(., datetime, contains("site_no"), ends_with(c("32400","30800","30600")), contains("00003"),-contains("cd")) %>%
filter(., site_no != "15s") %>% rename_at(vars(c(5,6,7)), ~ n) %>% rename_at(vars(contains("site")), ~ m)
s$datetime <- as.Date(as.numeric(s$datetime), origin = "1899-12-30")
return(s)}
y <- tidy(y); head(y, 6)
# Derive and plot daily dV and dQ
# Unit Conversion
convert_af_mcm = 1233.48/10.0^6 ## convert from Thousand acre feet to million m3
convert_cfs_mcmd = 3600.0*24.0*0.0283168/10.0^6 ## convert from cubic feet per second to million m3 per day;
y$V <- as.numeric(y$V) * convert_af_mcm;
y$Q_out <- as.numeric(y$Q_out) * convert_cfs_mcmd;
y$Q_in <- as.numeric(y$Q_in) * convert_cfs_mcmd;
# calculate mass balance
dV <- y$V %>% append(.,NA, 0); length(dV) <- nrow(y); dV <- y$V - dV
y <- y %>% mutate(.,dQ = Q_in - Q_out) %>% cbind(., dV)
# correlation of dV and dQ
cor(y$dV, y$dQ, use = "complete.obs")
# Plot dV vs dQ
range_limit <- max(abs(min(min(y$dV, na.rm = TRUE), min(y$dQ,na.rm = TRUE))),
abs(max(max(y$dV,na.rm = TRUE), max(y$dQ, na.rm = TRUE))))
ggplot(y) +
geom_point(mapping = aes(x = dQ, y = dV), color="darkgreen", shape= 17, alpha = 1/2, size = 3, na.rm = TRUE) +
labs(title ="Daily dQ vs dV", x = "dQ (m^3)", y = "dV (m^3)") +
xlim(-range_limit, range_limit) +  ylim(-range_limit, range_limit) +
geom_segment(aes(x = min(min(y$dV, na.rm = TRUE), min(y$dQ,na.rm = TRUE)),  y = min(min(y$dV, na.rm = TRUE), min(y$dQ,na.rm = TRUE)), xend = max(max(y$dV,na.rm = TRUE), max(y$dQ, na.rm = TRUE)), yend = max(max(y$dV,na.rm = TRUE), max(y$dQ, na.rm = TRUE))),
linetype = "dashed")
# Rescale: ET data
# This code chunk rescales the monthly average ET data to a time scale of daily.
head(et, 6)
# current unit million m3/month to million m3/day
# Note: different month has different num of days
et_mon <- et[start_mon:end_mon,]
# Unit Conversion for ET
convert_tcm_mcm = 0.001 ## convert from Thousand m3/month to million m3/month
et_mon$ET <- apply(et_mon[6:8],1 , mean)
et_mon <- et_mon %>%  select(., -c(2:8)) %>% mutate(., ET = ET*convert_tcm_mcm)
et_mon$Month <- as.Date(strptime(et_mon$Month, "%Y%m%d"))
et_day <- left_join(y, et_mon, by = c("datetime" = "Month")) %>% select(c("datetime","ET"))
et_day <- separate(et_day, 1, c("yr", "mon", "day"), convert = T)
for (i in start_yr:end_yr){
for (j in min(et_day[et_day$yr == i,]$mon):max(et_day[et_day$yr == i,]$mon)){
ET_daily <- et_day[et_day$yr== i & et_day$mon == j,][1,4]/nrow(et_day[et_day$yr== i & et_day$mon == j,])
for (k in 1:nrow(et_day[et_day$yr== i & et_day$mon == j,])){
et_day[et_day$yr== i & et_day$mon == j,][k,4] <- ET_daily
}
}
}
et_day <- et_day %>% unite("datetime", c("yr","mon", "day"), sep = "-"); et_day$datetime <- as.Date(et_day$datetime)
head(et_day, 6)
# Rescale: in situ data
# This code chunk rescales daily dQ, dV and dV added up with daily ET derived from previous codes by a time period of R (representing SWOT time scale)
# Using daily V and V_et rather than daily dV to calculate rescaled dV and V_et(in case some daily dV has missing in situ observations)
# Using dQ to calculate rescaled dQ
# Note: dQ in a unit of millin m3/day, dV in a unit of million m3
dy <- y %>% select(., datetime, dQ, dV, V) %>% left_join(., et_day) %>% mutate(., dQ_R = NA,V_R = NA, dV_R = NA, V_et = NA, V_et_R = NA, dV_et_R = NA)
# dQ_R
i <- 1;while (i * R < nrow(dy)){dy$dQ_R[R*i+1] <- sum(dy$dQ[(2+i*R-R):(i*R+1)], na.rm = T); i <- i+1}
# V_R
dy$V_R[1] <- dy$V[1];for (i in 2:(nrow(dy)-1)){
if(i %% R == 0) dy$V_R[i+1] <- dy$V[i+1]}
# dV_R
i <- 1;
while (i*R < nrow(dy)){dy$dV_R[i*R + 1] <- (dy$V_R[i*R +1] - dy$V_R[(i-1)*R + 1]); i <- i + 1}
# V_et
cumu_et <- data.frame(cumu = rep(NA, nrow(dy)));for (i in 1:nrow(dy)){  cumu_et[i,] <- sum(dy$ET[1:i], na.rm = T) }
dy$V_et <- cumu_et$cumu + dy$V
# V_et_R
dy$V_et_R[1] <- dy$V_et[1]
for (i in 2:(nrow(dy)-1)){if(i %% R == 0) dy$V_et_R[i+1] <- dy$V_et[i+1]}
# dV_et_R
i <- 1;while (i*R < nrow(dy)){dy$dV_et_R[i*R + 1] <- (dy$V_et_R[i*R +1] - dy$V_et_R[(i-1)*R + 1]); i <- i + 1}
dy <- dy %>%
select(datetime, dQ, dV, dV_R, dQ_R, dV_et_R)
head(dy,20)
head(y)
# Plotting dQ_R vs dV_R & dV_et_R
range_limit <- max(abs(min(min(dy$dV_R, na.rm = TRUE), min(dy$dQ_R, na.rm = TRUE))), abs(max(max(dy$dV_R, na.rm = TRUE), max(dy$dQ_R, na.rm = TRUE))))
ggplot(dy) +
geom_point(mapping = aes(x = dQ_R, y = dV_R), color="darkgreen", shape= 17, alpha = 1/2, size = 2, na.rm = TRUE) +
geom_segment(aes(x = min(min(dy$dV_R, na.rm = TRUE), min(dy$dQ_R, na.rm = TRUE)), y = min(min(dy$dV_R, na.rm = TRUE), min(dy$dQ_R, na.rm = TRUE)), xend = max(max(dy$dV_R, na.rm = TRUE), max(dy$dQ_R, na.rm = TRUE)), yend = max(max(dy$dV_R, na.rm = TRUE), max(dy$dQ_R, na.rm = TRUE))),linetype = "dashed") +
geom_point(mapping = aes(x = dQ_R, y = dV_et_R), color="red", shape= 17, alpha = 1/2, size = 2, na.rm = TRUE) +
labs(title =paste0("Storage-discharge balance for \n", s_name, " \n(Sampling Gap: ", R, " days)"), x = "dQ (million m^3)", y = "dV (million m^3)") +
xlim(-range_limit, range_limit) + ylim(-range_limit, range_limit) +
stat_smooth(aes(dy$dQ_R, dy$dV_et_R), method = "lm", col = "red", se = FALSE)
dV_et_R <- data.frame(datetime = y$datetime, dV_et_R = dy$dV_et_R) %>% xts(., order.by = .$datetime) %>% subset(., select = -datetime)
View(dy)
mx <- matrix(nrow = nrow(dy))
for (i in 1:R) {
## adding R-1 rows of NA of sequence
r <- rbind(seq(from = i, to = nrow(y), by = R), matrix(ncol = length(seq(from = i, to = nrow(y), by = R)), nrow = R-1))
## adding NA before the first value from the vector
r_NA <- c(rep(NA,i-1), r)
## length correction
length(r_NA) <- nrow(y)
mx <- as.data.frame(cbind(mx, r_NA)); names(mx)[i+1] <- paste0(c("dQ"), i)
}
# drop defaut column from df
mx <- subset(mx, select = -V1)
View(mx)
mx <- zoo(mx, dy$datetime)
dy <- dy  %>% xts(., order.by = .$datetime) %>% subset(., select = -c(datetime,ET,dV_et_R))
dy <- dy  %>% xts(., order.by = .$datetime) %>% subset(., select = -c(datetime,dV_et_R))
day <- seq(from = 1, to = nrow(dy)); dy <- cbind(dy, day)
dy <- merge(dy, mx)
for (i in 1:R){
for (j in 1:nrow(dy)){
if (!is.na(dy[j, i+5]))
dy[j, i+5] <- dy[j, 1]
}}
dy <- na_interpolation(dy, option = "linear") %>% subset(., select = -c(day,dQ,dV))
for (i in 1:R){
for (j in 1:((nrow(dy)-i)%/%R)){
a <- R*(j - 1) + i + 1; b <- j*R + i
dy[j*R + i, i + 2] <- sum(dy[,i+2][a:b])
dy[,i+2][a:(b-1)] <- NA
}
}
dy <- na_interpolation(dy, option = "linear") %>% merge(., dV_et_R)
dygraph(dy, main = paste0("Uncertainty Analysis for SWOT Observations of ", s_name, " (Sampling Gap: ", R, " days)"), ylab = "Storage change (km^3 / day)") %>%
dyRangeSelector() %>% dyLegend(width = 600) %>%
dySeries("dV_R", color = "seagreen",strokeWidth = 2, label = "Observed dV") %>% dySeries("dQ_R", color = "red",strokeWidth = 2, label = "Observed dQ") %>%
dySeries("dV_et_R", color = "green", drawPoints = TRUE, pointShape = "triangle", pointSize = "3", label = "dV corrected by reservoir evaporation") %>%
dyGroup(c(paste0(c("dQ"), 1:R)), color = rep("pink", R))
library("dplyr");library("rio");library("tidyverse");library("readxl");library("xts");library("dygraphs");library("imputeTS");library("ggplot2");library("zoo")
# Read and convert raw data
# Here we will read the long-term monthly evaportranspiration data developed by Dr. Gao Huilin for Possum Kingdom lake
s_name <- "Possum Kingdom Lk_09_10"
R <- 11 # sampling gap / temporal resolution
et_GRAND_ID <- 1176; et <- read.table(paste0("C:\\Users\\axin\\OneDrive - Kansas State University\\SWOT_from_Aote\\Supporting data\\ET\\Reservoir_evaporation721\\", et_GRAND_ID, ".txt"), header = T, stringsAsFactors = FALSE)
start_mon <- which(et$Month == 20081001, arr.ind = TRUE); end_mon <- which(et$Month == 20100901, arr.ind = TRUE)
start_yr <- 2008; end_yr <- 2010
head(et, 6)
y <- excel_sheets(paste0("C:/Users/axin/OneDrive - Kansas State University/SWOT_from_Aote/raw_data_by_num_of_streams/out_1_in_1/", s_name, ".xlsx")) %>%
map(~read_xlsx(paste0("C:/Users/axin/OneDrive - Kansas State University/SWOT_from_Aote/raw_data_by_num_of_streams/out_1_in_1/", s_name, ".xlsx"),.)) %>% ## convert multiple sheets from Excel into one sheet
data.frame()
head(y)
n = c("V", "Q_out", "Q_in"); m = c("site_V", "site_out", "site_in")
# function "tidy" extracts useful columns from the raw data frame and returns a single dataframe with date, site numbers, discharge measurements for inflow and outflow rivers and reservoir storage measurements
# 32400: observation at midnight; 00003: daily mean observation; 30600: observation at 6:00am; 30800: observation at 8:00am
tidy <- function(s){
s <- s %>% select(., datetime, contains("site_no"), ends_with(c("32400","30800","30600")), contains("00003"),-contains("cd")) %>%
filter(., site_no != "15s") %>% rename_at(vars(c(5,6,7)), ~ n) %>% rename_at(vars(contains("site")), ~ m)
s$datetime <- as.Date(as.numeric(s$datetime), origin = "1899-12-30")
return(s)}
y <- tidy(y); head(y, 6)
convert_af_mcm = 1233.48/10.0^6 ## convert from Thousand acre feet to million m3
convert_cfs_mcmd = 3600.0*24.0*0.0283168/10.0^6 ## convert from cubic feet per second to million m3 per day;
y$V <- as.numeric(y$V) * convert_af_mcm;
y$Q_out <- as.numeric(y$Q_out) * convert_cfs_mcmd;
y$Q_in <- as.numeric(y$Q_in) * convert_cfs_mcmd;
head(y, 6)
# calculate mass balance
dV <- y$V %>% append(.,NA, 0); length(dV) <- nrow(y); dV <- y$V - dV
y <- y %>% mutate(.,dQ = Q_in - Q_out) %>% cbind(., dV)
head(y, 6)
range_limit <- max(abs(min(min(y$dV, na.rm = TRUE), min(y$dQ,na.rm = TRUE))),
abs(max(max(y$dV,na.rm = TRUE), max(y$dQ, na.rm = TRUE))))
ggplot(y) +
geom_point(mapping = aes(x = dQ, y = dV), color="darkgreen", shape= 17, alpha = 1/2, size = 3, na.rm = TRUE) +
labs(title ="Daily dQ vs dV", x = "dQ (m^3)", y = "dV (m^3)") +
xlim(-range_limit, range_limit) +  ylim(-range_limit, range_limit) +
geom_segment(aes(x = min(min(y$dV, na.rm = TRUE), min(y$dQ,na.rm = TRUE)),  y = min(min(y$dV, na.rm = TRUE), min(y$dQ,na.rm = TRUE)), xend = max(max(y$dV,na.rm = TRUE), max(y$dQ, na.rm = TRUE)), yend = max(max(y$dV,na.rm = TRUE), max(y$dQ, na.rm = TRUE))),
linetype = "dashed")
head(et, 6)
et_mon <- et[start_mon:end_mon,]
et_mon
convert_tcm_mcm = 0.001
apply(et_mon[6:8],1 , mean)
et_mon$ET <- apply(et_mon[6:8],1 , mean)
et_mon
et_mon <- et_mon %>%  select(., -c(2:8)) %>% mutate(., ET = ET*convert_tcm_mcm)
et_mon$Month <- as.Date(strptime(et_mon$Month, "%Y%m%d"))
et_mon
et_day <- left_join(y, et_mon, by = c("datetime" = "Month")) %>% select(c("datetime","ET"))
et_day
et_day <- separate(et_day, 1, c("yr", "mon", "day"), convert = T)
et_day
for (i in start_yr:end_yr){
for (j in min(et_day[et_day$yr == i,]$mon):max(et_day[et_day$yr == i,]$mon)){
ET_daily <- et_day[et_day$yr == i & et_day$mon == j,][1,4]/nrow(et_day[et_day$yr == i & et_day$mon == j,])
for (k in 1:nrow(et_day[et_day$yr== i & et_day$mon == j,])){
et_day[et_day$yr== i & et_day$mon == j,][k,4] <- ET_daily
}
}
}
et_day
et_day <- et_day %>% unite("datetime", c("yr","mon", "day"), sep = "-")
et_day
et_day$datetime <- as.Date(et_day$datetime)
head(et_day, 6)
dy <- y %>% select(., datetime, dQ, dV, V) %>% left_join(., et_day) %>% mutate(., dQ_R = NA,V_R = NA, dV_R = NA, V_et = NA, V_et_R = NA, dV_et_R = NA)
head(dy)
# dQ_R
i <- 1;while (i * R < nrow(dy)){dy$dQ_R[R*i+1] <- sum(dy$dQ[(2+i*R-R):(i*R+1)], na.rm = T); i <- i+1}
head(dy)
head(dy,15)
dy$V_R[1] <- dy$V[1];for (i in 2:(nrow(dy)-1)){
if(i %% R == 0) dy$V_R[i+1] <- dy$V[i+1]}
head(dy,15)
i <- 1;
while (i*R < nrow(dy)){dy$dV_R[i*R + 1] <- (dy$V_R[i*R +1] - dy$V_R[(i-1)*R + 1]); i <- i + 1}
head(dy,15)
dy <- y %>% select(., datetime, dQ, dV, V) %>% left_join(., et_day) %>% mutate(., dQ_R = NA,V_R = NA, dV_R = NA, V_et = V + ET, V_et_R = NA, dV_et_R = NA)
head(dy,15)
# dQ_R
i <- 1;while (i * R < nrow(dy)){dy$dQ_R[R*i+1] <- sum(dy$dQ[(2+i*R-R):(i*R+1)], na.rm = T); i <- i+1}
# V_R (Storage measurement every R days)
dy$V_R[1] <- dy$V[1];for (i in 2:(nrow(dy)-1)){
if(i %% R == 0) dy$V_R[i+1] <- dy$V[i+1]}
# dV_R (Storage difference every R days)
i <- 1;
while (i*R < nrow(dy)){dy$dV_R[i*R + 1] <- (dy$V_R[i*R +1] - dy$V_R[(i-1)*R + 1]); i <- i + 1}
head(dy,15)
dy$V_et_R[1] <- dy$V_et[1]
for (i in 2:(nrow(dy)-1)){if(i %% R == 0) dy$V_et_R[i+1] <- dy$V_et[i+1]}
head(dy,15)
i <- 1;while (i*R < nrow(dy)){dy$dV_et_R[i*R + 1] <- (dy$V_et_R[i*R +1] - dy$V_et_R[(i-1)*R + 1]); i <- i + 1}
head(dy,15)
range_limit <- max(abs(min(min(dy$dV_R, na.rm = TRUE), min(dy$dQ_R, na.rm = TRUE))), abs(max(max(dy$dV_R, na.rm = TRUE), max(dy$dQ_R, na.rm = TRUE))))
ggplot(dy) +
geom_point(mapping = aes(x = dQ_R, y = dV_R), color="darkgreen", shape= 17, alpha = 1/2, size = 2, na.rm = TRUE) +
geom_segment(aes(x = min(min(dy$dV_R, na.rm = TRUE), min(dy$dQ_R, na.rm = TRUE)), y = min(min(dy$dV_R, na.rm = TRUE), min(dy$dQ_R, na.rm = TRUE)), xend = max(max(dy$dV_R, na.rm = TRUE), max(dy$dQ_R, na.rm = TRUE)), yend = max(max(dy$dV_R, na.rm = TRUE), max(dy$dQ_R, na.rm = TRUE))),linetype = "dashed") +
geom_point(mapping = aes(x = dQ_R, y = dV_et_R), color="red", shape= 17, alpha = 1/2, size = 2, na.rm = TRUE) +
labs(title =paste0("Storage-discharge balance for \n", s_name, " \n(Sampling Gap: ", R, " days)"), x = "dQ (million m^3)", y = "dV (million m^3)") +
xlim(-range_limit, range_limit) + ylim(-range_limit, range_limit) +
stat_smooth(aes(dy$dQ_R, dy$dV_et_R), method = "lm", col = "red", se = FALSE)
dV_et_R <- data.frame(datetime = y$datetime, dV_et_R = dy$dV_et_R) %>% xts(., order.by = .$datetime) %>% subset(., select = -datetime)
mx <- matrix(nrow = nrow(dy))
for (i in 1:R) {
## adding R-1 rows of NA of sequence
r <- rbind(seq(from = i, to = nrow(y), by = R), matrix(ncol = length(seq(from = i, to = nrow(y), by = R)), nrow = R-1))
## adding NA before the first value from the vector
r_NA <- c(rep(NA,i-1), r)
## length correction
length(r_NA) <- nrow(y)
mx <- as.data.frame(cbind(mx, r_NA)); names(mx)[i+1] <- paste0(c("dQ"), i)
}
mx
mx <- subset(mx, select = -V1) # View(mx)
mx <- zoo(mx, dy$datetime)
mx
dy <- dy  %>% xts(., order.by = .$datetime) %>% subset(., select = -c(datetime,dV_et_R))
head(dy,20)
day <- seq(from = 1, to = nrow(dy)); dy <- cbind(dy, day)
head(dy,20)
dy <- merge(dy, mx)
head(dy,20)
dy <- y %>% select(., datetime, dQ, dV, V) %>% left_join(., et_day) %>% mutate(., dQ_R = NA,V_R = NA, dV_R = NA, V_et = V + ET, V_et_R = NA, dV_et_R = NA)
head(dy,15)
# dQ_R
i <- 1;while (i * R < nrow(dy)){dy$dQ_R[R*i+1] <- sum(dy$dQ[(2+i*R-R):(i*R+1)], na.rm = T); i <- i+1}
# V_R (Storage measurement every R days)
dy$V_R[1] <- dy$V[1];for (i in 2:(nrow(dy)-1)){
if(i %% R == 0) dy$V_R[i+1] <- dy$V[i+1]}
# dV_R (Storage difference every R days)
i <- 1;
while (i*R < nrow(dy)){dy$dV_R[i*R + 1] <- (dy$V_R[i*R +1] - dy$V_R[(i-1)*R + 1]); i <- i + 1}
# V_et_R
dy$V_et_R[1] <- dy$V_et[1]
for (i in 2:(nrow(dy)-1)){if(i %% R == 0) dy$V_et_R[i+1] <- dy$V_et[i+1]}
# dV_et_R
i <- 1;while (i*R < nrow(dy)){dy$dV_et_R[i*R + 1] <- (dy$V_et_R[i*R +1] - dy$V_et_R[(i-1)*R + 1]); i <- i + 1}
dy <- dy %>%
select(datetime, dQ, dV, dV_R, dQ_R, dV_et_R)
dV_et_R <- data.frame(datetime = y$datetime, dV_et_R = dy$dV_et_R) %>% xts(., order.by = .$datetime) %>% subset(., select = -datetime)
mx <- matrix(nrow = nrow(dy))
for (i in 1:R) {
## adding R-1 rows of NA of sequence
r <- rbind(seq(from = i, to = nrow(y), by = R), matrix(ncol = length(seq(from = i, to = nrow(y), by = R)), nrow = R-1))
## adding NA before the first value from the vector
r_NA <- c(rep(NA,i-1), r)
## length correction
length(r_NA) <- nrow(y)
mx <- as.data.frame(cbind(mx, r_NA)); names(mx)[i+1] <- paste0(c("dQ"), i)
}
# drop defaut column from df
mx <- subset(mx, select = -V1) # View(mx)
# convert mx to time series object
mx <- zoo(mx, dy$datetime)
dy <- dy  %>% xts(., order.by = .$datetime) %>% subset(., select = -c(datetime,dV_et_R,V,ET,V_R,V_et,V_et_R))
dy <- y %>% select(., datetime, dQ, dV, V) %>% left_join(., et_day) %>% mutate(., dQ_R = NA,V_R = NA, dV_R = NA, V_et = V + ET, V_et_R = NA, dV_et_R = NA)
head(dy,15)
# dQ_R
i <- 1;while (i * R < nrow(dy)){dy$dQ_R[R*i+1] <- sum(dy$dQ[(2+i*R-R):(i*R+1)], na.rm = T); i <- i+1}
# V_R (Storage measurement every R days)
dy$V_R[1] <- dy$V[1];for (i in 2:(nrow(dy)-1)){
if(i %% R == 0) dy$V_R[i+1] <- dy$V[i+1]}
# dV_R (Storage difference every R days)
i <- 1;
while (i*R < nrow(dy)){dy$dV_R[i*R + 1] <- (dy$V_R[i*R +1] - dy$V_R[(i-1)*R + 1]); i <- i + 1}
# V_et_R
dy$V_et_R[1] <- dy$V_et[1]
for (i in 2:(nrow(dy)-1)){if(i %% R == 0) dy$V_et_R[i+1] <- dy$V_et[i+1]}
# dV_et_R
i <- 1;while (i*R < nrow(dy)){dy$dV_et_R[i*R + 1] <- (dy$V_et_R[i*R +1] - dy$V_et_R[(i-1)*R + 1]); i <- i + 1}
dy <- dy %>%
select(datetime, dQ, dV, dV_R, dQ_R, dV_et_R)
dV_et_R <- data.frame(datetime = y$datetime, dV_et_R = dy$dV_et_R) %>% xts(., order.by = .$datetime) %>% subset(., select = -datetime)
mx <- matrix(nrow = nrow(dy))
for (i in 1:R) {
## adding R-1 rows of NA of sequence
r <- rbind(seq(from = i, to = nrow(y), by = R), matrix(ncol = length(seq(from = i, to = nrow(y), by = R)), nrow = R-1))
## adding NA before the first value from the vector
r_NA <- c(rep(NA,i-1), r)
## length correction
length(r_NA) <- nrow(y)
mx <- as.data.frame(cbind(mx, r_NA)); names(mx)[i+1] <- paste0(c("dQ"), i)
}
# drop defaut column from df
mx <- subset(mx, select = -V1) # View(mx)
# convert mx to time series object
mx <- zoo(mx, dy$datetime)
dy <- dy  %>% xts(., order.by = .$datetime) %>% subset(., select = -c(datetime,dV_et_R))
day <- seq(from = 1, to = nrow(dy)); dy <- cbind(dy, day)
head(dy,15)
dy <- merge(dy, mx)
head(dy,15)
for (i in 1:R){
for (j in 1:nrow(dy)){
if (!is.na(dy[j, i+5]))
dy[j, i+5] <- dy[j, 1]
}}
head(dy,15)
dy <- na_interpolation(dy, option = "linear") %>% subset(., select = -c(day,dQ,dV))
head(dy,15)
for (i in 1:R){
for (j in 1:((nrow(dy)-i)%/%R)){
a <- R*(j - 1) + i + 1; b <- j*R + i
dy[j*R + i, i + 2] <- sum(dy[,i+2][a:b])
dy[,i+2][a:(b-1)] <- NA
}
}
head(dy,15)
dy <- na_interpolation(dy, option = "linear") %>% merge(., dV_et_R)
head(dy,15)
dygraph(dy, main = paste0("Uncertainty Analysis for SWOT Observations of ", s_name, " (Sampling Gap: ", R, " days)"), ylab = "Storage change (km^3 / day)") %>%
dyRangeSelector() %>% dyLegend(width = 600) %>%
dySeries("dV_R", color = "seagreen",strokeWidth = 2, label = "Observed dV") %>% dySeries("dQ_R", color = "red",strokeWidth = 2, label = "Observed dQ") %>%
dySeries("dV_et_R", color = "green", drawPoints = TRUE, pointShape = "triangle", pointSize = "3", label = "dV corrected by reservoir evaporation") %>%
dyGroup(c(paste0(c("dQ"), 1:R)), color = rep("pink", R))
