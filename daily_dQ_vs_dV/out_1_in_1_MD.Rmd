---
title: "Testing the water balance between reservoirs and their connected rivers at a SWOT's operating time scale"
author: "Aote"
date: "5/12/2020"
output: html_document
---
*[NOTES: THESE CODES WERE DEVELOPED FOR 'TESTING A PROPOSED ALGORITHM FOR ESTIMATING WATER DISCHARGE
 AT RIVER-RESERVOIR INTERFACE: POTENTIAL APPLICATIONS FOR THE SURFACE WATER AND OCEAN TOPOGRAPHY SATELLITE MISSION' PROJECT]*

***

**This markdown file is demonstrated on Possum Kingdom Lake during water year 2009 and 2010.**

**Variable R which represents the sampling gaps is set to 10 days in this markdown file (see tab *Read and Convert Raw Data*).**

***

There are 7 tabs in this markdown file. Although *Rescale* sections include rescaling raw datasets into a time scale of R days, but they do not include conversion into monthly time scale. The tab *Additional: Daily to Monthly* addresses this conversion.

# Graduate Project{.tabset .tabset-fade.tabset-pills}

## Description of my Graduate Project
**Abstract**

River discharge is an essential component of the water cycle. The forthcoming Surface Water and Ocean Topography (SWOT) satellite mission will enable direct measurements of water surface width (W), height (h) and slope (S) of major rivers globally. With SWOT, the estimation of discharge in ungauged rivers through remote sensing becomes possible with an unprecedented spatial coverage. However, a reliable method remains unavailable as the estimates from existing methods lack adequate accuracies. A feasible approach needs to be developed prior to the launch of SWOT. My study will test a proposed discharge estimation algorithm by incorporating the water mass balance between open water storages (i.e., lakes and reservoirs) and their connecting rivers. Our preliminary results suggest that the conservation of water mass can be observed in lakes and reservoirs located in a wide range of hydroclimate regions. This relationship will be incorporated as additional a priori data to discharge estimation besides the hydraulic variables that can be directly measured by SWOT. The potential applicability of the proposed algorithm will be investigated on 31 rivers across the contiguous United States with accessible gage measurements.

In this graduate project, I will test the water balance between reservoirs and their connected rivers at a SWOT's operating time scale. As the discharge difference between upstream and downstream rivers may not be the only control of reservoir storage change, reservoir surface evaporation is considered and incorporated in this project. The ET contribution is quantified using the CONUS Reservoir Evaporation Dataset (CRED) (Zhao & Gao, 2019). Future efforts for these codes will include addressing lateral inflows and adding statistical metrices for assessment.

***

**Background**

Considered as a naturally circulating resource, water is constantly being consumed and recharged around the Earth. Streams and rivers route approximately 40% of the global precipitation over the continents back to the ocean (Oki & Kanae, 2006). The quantification of water resources in river channels is crucial as river discharge contains valuable information for water resources assessment, water scarcity evaluation and climate change projection (Alsdorf & Lettenmaier, 2003).

The gaging network is not replaceable as it provides temporally continuous in situ observations of critical hydraulic variables. While providing accurate measurements, gage measurements lack spatial consistency and remains unavailable in many regions worldwide due to financial and political constraints. We have witnessed a rapid development of remote sensing technology in the last decade (Qu, Gao, Kafatos, Murphy, & Salomonson, 2006). With SWOT, synchronous space-borne measurements of water extents, surface height, and slope will become available for rivers wider than 30 m and lakes/reservoirs larger than 62,500 m2. It is capable of providing at least one image in most regions of the world in a sub-monthly interval while still maintaining a slope accuracy at a decimeter level (1 cm/km) for river widths greater than 100 m (Biancamaria, Lettenmaier, & Pavelsky, 2016).

Current river discharge algorithms remain an explorative ground for research. Extensive efforts have been made in the past decade to develop approaches to estimate river discharge using hydraulic variables that are directly measurable by SWOT. Existing methods lack consistency in providing accurate discharge estimates for rivers across different hydroclimate conditions (Durand et al., 2016; Durand et al., 2014; Hagemann, Gleason, & Durand, 2017). Furthermore, these methods are inadequate to estimate water discharge at lake-river interface. It has become imperative for a reliable estimation method to be developed prior to the launch of SWOT in September, 2021.

***

**Describe how your specific research will contribute to addressing that problem or challenge**

The objective of this research is to develop a globally deployable river discharge estimation algorithm using the direct measurements by SWOT. Streamflow monitoring enhances decision-making. This algorithm specifically measures the river discharge at the interface between rivers and open water storages (i.e., lakes and reservoirs). Water discharge at the river-storage interface, such as reservoir inflow and outflow, is critical to water resource management, and has not been adequately exploited by the existing discharge algorithms.

Manningâ€™s equation is a semi-empirical hydraulic equation, providing a reliable approach to calculate river discharge. Statistical approaches will then be utilized in the algorithm to invert bathymetry and riverbed roughness coefficient which are unavailable through spatial observations (Durand et al., 2014)

Previous studies suggest future efforts to consider incorporating more ancillary data to improve the performance of discharge estimation (Durand et al., 2016). The construction of dams and their reservoir impoundment fragments the once free-flowing rivers. By number, 77% of the rivers connected to the ocean with a river length greater than 1000 km has been categorized as non-free-flowing rivers (Grill et al., 2019). Regardless of the ecological and environmental impacts on the riparian system, the mass balance between reservoirs and their connecting rivers could be incorporated into discharge estimation. The addition of more a priori data as another constraint could potentially improve the effectiveness of the algorithm (Durand et al., 2016).

***

**Data collection**

The mass balance between lakes and their connected rivers was tested using in situ data for 31 candidates across the contiguous United States (CONUS). Preliminary results are encouraging by indicating that the discharge difference between upstream and downstream rivers is the first order control on the volume changes of lakes and reservoirs. This relationship is spatially prevalent, and thus justifies our methodological premise that the water mass balance between rivers and lakes/reservoirs can be applied to improving the estimation of discharge at their interface (such as reservoir inflows and outflows). 

Thirty-one reservoirs with gage inflow, outflow, and reservoir storage measurements, were selected across a wide range of hydrologic landscape regions in CONUS (Wolock, 2003). Among the 31 candidates, 19 of them can be observed by SWOT. This identification was made with the assistance of the Global River Widths from Landsat (GRWL) database which contains rivers of at least 30 m wide (Allen & Pavelsky, 2018). To test the mass balance of these candidates, gage data were downloaded from the USGS National Water Information System. As the candidates are distributed across water-limited and energy-limited regions, discharge difference may not be the only control on the reservoir storage changes. The potential impact of meteorological contributions above lake surfaces and lateral inflows will be considered in this study. These two contributions will be quantified using the CONUS Reservoir Evaporation Dataset (CRED) (Zhao & Gao, 2019), and Global Reach-level A priori Discharge Estimates for SWOT (GRADES) dataset (Lin et al., 2019).

The phase of testing our algorithm after development requires sensible hydraulic variables from remote sensing as input to our algorithm. AirSWOT is an instrument for supporting the SWOT mission which is designed to make interferometric measurements similar to those that will be made in space by SWOT. Our future research will require us to seek potential cooperation with the SWOT science team in order to obtain AirSWOT measurements (Tuozzolo et al., 2019).

***

**Discussion of anticipated outcomes and results**

Our method is novel and has not been proposed by any other scholar yet. If our algorithm yields a high accuracy on discharge estimation, our measurements would benefit scientific research focusing on habitat conservation for aquatic species, water management for agriculture, industrial water use planning, and natural freshwater assessment. Coupled with the georeferenced global dataset of dams and reservoirs, the new algorithm will provide the scientific community with improved river discharge estimates that are needed to calibrate and constrain the hydrologic models to forecast effects of future change in the hydrologic cycle (Durand et al., 2016). In addition, scientists can better understand the current global water storage and dynamics given the possible acceleration of water cycle due to global warming (Oki & Kanae, 2006).

The proposed algorithm will provide important scientific guidance for policy makers to better regulate water distribution and ensure the stability of infrastructure development, food supply and public health. Current and future freshwater resource statuses can be illustrated with models that can describe more accurate and detailed hydrologic processes with our estimates. As a key component of the water cycle, river discharge has been vital to the development of human civilization for thousands of years. In arid and semiarid regions, a lack of understanding of river discharge as a major part of natural freshwater resources would aggravate more stress on the already severe freshwater condition. In regions with excessive precipitation, a high temporal discharge variability may also trigger severe droughts in the crop-growing season.

## Install Packages
```{r install, cache= T}
# installing and loading packages
# install.packages(c("dplyer", "tidyverse","ggplot2", "xts","dygraphs","imputeTS"))
library("dplyr");library("rio");library("tidyverse");library("readxl");library("xts");library("dygraphs");library("imputeTS");library("ggplot2");library("zoo")
```

## Read and Convert Raw Data
- Here we will read the long-term monthly evaportranspiration data developed by Dr. Gao Huilin for Possum Kingdom lake
```{r read ET data, cache = T}
s_name <- "Possum Kingdom Lk_09_10"
R <- 10 # sampling gap / temporal resolution
et_GRAND_ID <- 1176; et <- read.table(paste0("C:\\Users\\axin\\OneDrive - Kansas State University\\SWOT_from_Aote\\Supporting data\\ET_candidates\\", et_GRAND_ID, ".txt"), header = T, stringsAsFactors = FALSE) 
start_mon <- which(et$Month == 20081001, arr.ind = TRUE); end_mon <- which(et$Month == 20100901, arr.ind = TRUE)
start_yr <- 2008; end_yr <- 2010 

head(et, 6)
```

***

- In the next chunk, the in situ data are read which includes the discharge measurements for inflow and outflow rivers and reservoir storage measurements all at a daily basis. Data acquired from USGS NWIS.
```{r read in situ data, cache = T}
y <- excel_sheets(paste0("C:/Users/axin/OneDrive - Kansas State University/SWOT_from_Aote/raw_data_by_num_of_streams/out_1_in_1/", s_name, ".xlsx")) %>% 
  map(~read_xlsx(paste0("C:/Users/axin/OneDrive - Kansas State University/SWOT_from_Aote/raw_data_by_num_of_streams/out_1_in_1/", s_name, ".xlsx"),.)) %>%
  data.frame()

## tidy data
n = c("V", "Q_out", "Q_in"); m = c("site_V", "site_out", "site_in")
tidy <- function(s){ 
  s <- s %>% select(., datetime, contains("site_no"), ends_with(c("32400","30800","30600")), contains("00003"),-contains("cd")) %>%
             filter(., site_no != "15s") %>% rename_at(vars(c(5,6,7)), ~ n) %>% rename_at(vars(contains("site")), ~ m) 
  s$datetime <- as.Date(as.numeric(s$datetime), origin = "1899-12-30")
  return(s)}

y <- tidy(y);head(y, 6)
```

## Derive and Plot Daily dV and dQ
```{r , cache=T}
## Unit Conversion
convert_af_mcm = 1233.48/10.0^6 #convert from Thousand acre feet to million m3
convert_cfs_mcmd = 3600.0*24.0*0.0283168/10.0^6 #convert from cubic feet per second to million m3 per day;
y$V <- as.numeric(y$V) * convert_af_mcm; 
y$Q_out <- as.numeric(y$Q_out) * convert_cfs_mcmd; 
y$Q_in <- as.numeric(y$Q_in) * convert_cfs_mcmd; 

## calculate mass balance
dV <- y$V %>% append(.,NA, 0); length(dV) <- nrow(y); dV <- y$V - dV
y <- y %>% mutate(.,dQ = Q_in - Q_out) %>% cbind(., dV)

## correlation of dV and dQ
cor(y$dV, y$dQ, use = "complete.obs")

## Plot dV vs dQ
range_limit <- max(abs(min(min(y$dV, na.rm = TRUE), min(y$dQ,na.rm = TRUE))), 
                   abs(max(max(y$dV,na.rm = TRUE), max(y$dQ, na.rm = TRUE))))
         
ggplot(y) +
  geom_point(mapping = aes(x = dQ, y = dV), color="darkgreen", shape= 17, alpha = 1/2, size = 3, na.rm = TRUE) +
  labs(title ="Daily dQ vs dV", x = "dQ (m^3)", y = "dV (m^3)") +
  xlim(-range_limit, range_limit) +  ylim(-range_limit, range_limit) +
  geom_segment(aes(x = min(min(y$dV, na.rm = TRUE), min(y$dQ,na.rm = TRUE)),  y = min(min(y$dV, na.rm = TRUE), min(y$dQ,na.rm = TRUE)), xend = max(max(y$dV,na.rm = TRUE), max(y$dQ, na.rm = TRUE)), yend = max(max(y$dV,na.rm = TRUE), max(y$dQ, na.rm = TRUE))),
               linetype = "dashed")
```

## Rescale: ET data
- This code chunk rescales the monthly average ET data to a time scale of daily.
```{r ET, cache=T}
head(et, 6)
## rescale ET data from monthly to daily
## current unit million m3/month to million m3/day
## Note different month has different num of days
et_mon <- et[start_mon:end_mon,]
## Unit Conversion for ET
convert_tcm_mcm = 0.001 #convert from Thousand m3/month to million m3/month
et_mon$ET <- apply(et_mon[6:8],1 , mean)
et_mon <- et_mon %>%  select(., -c(2:8)) %>% mutate(., ET = ET*convert_tcm_mcm)

et_mon$Month <- as.Date(strptime(et_mon$Month, "%Y%m%d"))
et_day <- left_join(y, et_mon, by = c("datetime" = "Month")) %>% select(c("datetime","ET"))
et_day <- separate(et_day, 1, c("yr", "mon", "day"), convert = T)

for (i in start_yr:end_yr){
  for (j in min(et_day[et_day$yr == i,]$mon):max(et_day[et_day$yr == i,]$mon)){
    ET_daily <- et_day[et_day$yr== i & et_day$mon == j,][1,4]/nrow(et_day[et_day$yr== i & et_day$mon == j,])
    for (k in 1:nrow(et_day[et_day$yr== i & et_day$mon == j,])){
      et_day[et_day$yr== i & et_day$mon == j,][k,4] <- ET_daily
    }
  }
}
et_day <- et_day %>% unite("datetime", c("yr","mon", "day"), sep = "-"); et_day$datetime <- as.Date(et_day$datetime)
head(et_day, 6)
```

## Rescale: In situ data 
- This code chunk rescales daily dQ, dV and dV added up with daily ET derived from previous codes by a time period of R (representing SWOT time scale)
```{r In situ, cache=T}
## Rescale daily dQ and dV by a time period of R (representing SWOT time scale)
## dQ in a unit of millin m3/day, dV in a unit of million m3
N <- rep(NA, R-1)
dy <- y %>% select(., datetime, dQ, dV) %>% left_join(., et_day) %>% mutate(., dV_et =  dV + ET, dV_R = NA, dQ_R = NA, dV_et_R = NA) 

## dV_R == V(R*(n+1))-V(R*n)
i <- 1
while (i * R < nrow(dy)){dy$dV_R[R*i+1] <- sum(dy$dV[(2+i*R-R):(i*R+1)], na.rm = T); i <- i+1}
## dQ_R
i <- 1
while (i * R < nrow(dy)){dy$dQ_R[R*i+1] <- sum(dy$dQ[(2+i*R-R):(i*R+1)], na.rm = T); i <- i+1}
## dV_et_R
i <- 1
while (i * R < nrow(dy)){dy$dV_et_R[R*i+1] <- sum(dy$dV_et[(2+i*R-R):(i*R+1)], na.rm = T); i <- i+1}

## Plotting dQ_R vs dV_R & dV_et_R
range_limit <- max(abs(min(min(dy$dV_R, na.rm = TRUE), min(dy$dQ_R, na.rm = TRUE))), abs(max(max(dy$dV_R, na.rm = TRUE), max(dy$dQ_R, na.rm = TRUE))))
ggplot(dy, aes(dQ_R, dV_R, dV_et_R)) +
  geom_point(mapping = aes(x = dQ_R, y = dV_R), color="darkgreen", shape= 17, alpha = 1/2, size = 2, na.rm = TRUE) +
  geom_segment(aes(x = min(min(dy$dV_R, na.rm = TRUE), min(dy$dQ_R, na.rm = TRUE)), y = min(min(dy$dV_R, na.rm = TRUE), min(dy$dQ_R, na.rm = TRUE)), xend = max(max(dy$dV_R, na.rm = TRUE), max(dy$dQ_R, na.rm = TRUE)), yend = max(max(dy$dV_R, na.rm = TRUE), max(dy$dQ_R, na.rm = TRUE))),linetype = "dashed") +
  geom_point(mapping = aes(x = dQ_R, y = dV_et_R), color="red", shape= 17, alpha = 1/2, size = 2, na.rm = TRUE) +
  labs(title =paste0("Storage-discharge balance for \n", s_name, " \n(Sampling Gap: ", R, " days)"), x = "dQ (million m^3)", y = "dV (million m^3)") +
  xlim(-range_limit, range_limit) + ylim(-range_limit, range_limit)

dV_et_R <- data.frame(datetime = y$datetime, dV_et_R = dy$dV_et_R) %>% xts(., order.by = .$datetime) %>% subset(., select = -datetime)
head(dy, 22)
```
## Additional: Daily to Monthly data
- This code chunk converts daily in situ data to a time scale of month. The challenge here is to realize that each month has different number of days and take account of the difference when deriving monthly data.
```{r dmon, cache=T}
#rescale in situ discharge and storage data from daily to monthly 
dy_mon <- y %>% select(., datetime, dQ, dV) %>%  mutate(., dV_R = NA, dQ_R = NA); dy_mon <- separate(dy_mon, 1, c("yr", "mon", "day"), convert = T)
 
for (i in start_yr:end_yr){
 for (j in min(dy_mon[dy_mon$yr == i,]$mon):max(dy_mon[dy_mon$yr == i,]$mon)){
    dy_mon[dy_mon$yr== i & dy_mon$mon == j,][1,6] <- sum(dy_mon$dV[dy_mon$yr == i & dy_mon$mon == j], na.rm = T)
    dy_mon[dy_mon$yr== i & dy_mon$mon == j,][1,7] <- sum(dy_mon$dQ[dy_mon$yr == i & dy_mon$mon == j], na.rm = T)
  }}

dy_mon <- dy_mon %>% unite("datetime", c("yr","mon", "day"), sep = "-"); dy_mon$datetime <- as.Date(dy_mon$datetime)
dy_mon <- left_join(dy_mon, et_mon, by = c("datetime"="Month")) %>% mutate(., dV_et_R = dV_R + ET)
dV_et_mon <- data.frame(dV_et_R = dy_mon$dV_et_R) 

# see monthly results of dV_R, dQ_R and dV_et_R
head(dy_mon[!is.na(dy_mon$dV_et_R),])

# Plot dV vs dQ
range_limit <- max(abs(min(min(dy_mon$dV_R, na.rm = TRUE), min(dy_mon$dQ_R,na.rm = TRUE))), abs(max(max(dy_mon$dV_R,na.rm = TRUE), max(dy_mon$dQ_R, na.rm = TRUE))))
ggplot(dy_mon) +
  geom_point(mapping = aes(x = dQ_R, y = dV_R), color="darkgreen", shape= 17, alpha = 1/2, size = 3, na.rm = TRUE) +
  labs(title =paste0("Storage-discharge balance for \n", s_name, " \n(Sampling Gap: monthly)"), x = "dQ (million m^3)", y = "dV (million m^3)") +
  xlim(-range_limit, range_limit) + ylim(-range_limit, range_limit) +
  geom_segment(aes(x = min(min(dy_mon$dV_R, na.rm = TRUE), min(dy_mon$dQ_R, na.rm = TRUE)), y = min(min(dy_mon$dV_R, na.rm = TRUE), min(dy_mon$dQ_R, na.rm = TRUE)), xend = max(max(dy_mon$dV_R, na.rm = TRUE), max(dy_mon$dQ_R, na.rm = TRUE)), yend = max(max(dy_mon$dV_R, na.rm = TRUE), max(dy_mon$dQ_R, na.rm = TRUE))),linetype = "dashed") +
  geom_point(mapping = aes(x = dQ_R, y = dV_et_R), color="green", shape= 17, alpha = 1/2, size = 3, na.rm = TRUE)

### Plot hydrograph
dy_mon <- dy_mon %>% select(., -c(dQ, dV, ET, dV_et_R)) %>% na_interpolation(., option = "linear") 
xx <- xts(dy_mon[,1], order.by = dy_mon$datetime); 
xx <- cbind(xx, dy_mon$dV_R,dy_mon$dQ_R, dV_et_mon$dV_et_R) %>% subset(., select = -xx); 
colnames(xx) <- c("dV_R", "dQ_R", "dV_et_R")

dygraph(xx, main = paste0("Uncertainty Analysis for SWOT Observations of ", s_name, " (Sampling Gap: monthly)"), ylab = "Storage change (km^3 / day)") %>% 
  dyRangeSelector() %>% 
  dyLegend(width = 600) %>% 
  dySeries("dV_R", color = "seagreen",strokeWidth = 1) %>% 
  dySeries("dQ_R", color = "red",strokeWidth = 1) %>% 
  dySeries("dV_et_R", color = "green", pointShape = "triangle", pointSize = "3") 

 
 dev.off()
 
```

## Uncertainty Analysis due to Sampling Gaps
```{r uncertainty, cache=T}
# Uncertainty Analysis due to sampling gaps by SWOT
## A loop to create a matrix (mx) of sampling days in different scenarios
mx <- matrix(nrow = nrow(dy))
for (i in 1:R) {
  ### adding R-1 rows of NA of sequence
  r <- rbind(seq(from = i, to = nrow(y), by = R), matrix(ncol = length(seq(from = i, to = nrow(y), by = R)), nrow = R-1)) 
  ### adding NA before the first value from the vector
  r_NA <- c(rep(NA,i-1), r)
  ### length correction 
  length(r_NA) <- nrow(y)
  
  mx <- as.data.frame(cbind(mx, r_NA)); names(mx)[i+1] <- paste0(c("dQ"), i)
}
### drop defaut column from df
mx <- subset(mx, select = -V1) # View(mx)
### convert mx to time series object
mx <- zoo(mx, dy$datetime) 
dy <- dy  %>% xts(., order.by = .$datetime) %>% subset(., select = -c(datetime,ET,dV_et,dV_et_R))
### add # of days as a new column
day <- seq(from = 1, to = nrow(dy)); dy <- cbind(dy, day)  
### join indices of sampling days (mx) to dy 
dy <- merge(dy, mx)

## Due to sampling gaps, dQ is remotely measured every R days by SWOT.
## Here we simulate the effects of sampling gaps and assume dQ on days without measurements, 
## takes on a linear distribution btw sampling days.

### add dQ to sampling days
for (i in 1:R){
  for (j in 1:nrow(dy)){
    if (!is.na(dy[j, i+5]))
      dy[j, i+5] <- dy[j, 1]
  }}
### Interpolate the discharge differences dQ on days without measurements (linear)
### Assume the daily volumes of discharge take on a linear variation
dy <- na_interpolation(dy, option = "linear") %>% subset(., select = -c(day,dQ,dV))
### Rescale/Aggregate daily dQ and dV by a time period of R (representing SWOT time scale)
### dQ in a unit of millin m3/day
for (i in 1:R){
  for (j in 1:((nrow(dy)-i)%/%R)){
    a <- R*(j - 1) + i + 1; b <- j*R + i
    dy[j*R + i, i + 2] <- sum(dy[,i+2][a:b])
    dy[,i+2][a:(b-1)] <- NA
  }
}
### interpolate the volume variation from discharge difference
### Here according to our assumption, the daily volume of discharge take on a linear variation
dy <- na_interpolation(dy, option = "linear") %>% merge(., dV_et_R)
###############################################################################################################
## upper and lower error bars
dy_error <- dy %>% subset(., select = -c(dQ_R,dV_R,dV_et_R)) %>% fortify.zoo() %>% select(-Index)
  
### calculate highest and lowest values for each scenarios/row
dy_error$max <- apply(X = dy_error, MARGIN=1, FUN=max)
dy_error$min <- apply(X = dy_error, MARGIN=1, FUN=min)

### dy_2 
 dy_error <- dy_error %>% select(max, min) %>% zoo(., order.by = y$datetime)
 dy_2 <- dy %>% subset(., select= c(dQ_R,dV_R,dV_et_R)) %>% merge(., dy_error)
###############################################################################################################
# Plot hydrograph with upper/lower bars
 dygraph(dy_2, main = paste0("Uncertainty Analysis for SWOT Observations of ", s_name, " (Sampling Gap: ", R, " days)"), ylab = "Storage change (km^3 / day)") %>% 
  dyRangeSelector() %>% dyLegend(width = 600) %>%
  dySeries(c("min", "dQ_R", "max"), label = "Observed dQ", color = "red") %>% dySeries("dV_R", label = "Observed dV", color = "seagreen") %>% 
  dySeries("dV_et_R", color = "green", drawPoints = TRUE, pointShape = "triangle", pointSize = "3", label = "dV corrected by reservoir evaporation") %>%
  dyHighlight(highlightCircleSize = 5, highlightSeriesBackgroundAlpha = 0.2, hideOnMouseOut = TRUE) 

# Plot hydrograph showing uncertainty as lines
 dygraph(dy, main = paste0("Uncertainty Analysis for SWOT Observations of ", s_name, " (Sampling Gap: ", R, " days)"), ylab = "Storage change (km^3 / day)") %>% 
  dyRangeSelector() %>% dyLegend(width = 600) %>%
  dySeries("dV_R", color = "seagreen",strokeWidth = 2, label = "Observed dV") %>% dySeries("dQ_R", color = "red",strokeWidth = 2, label = "Observed dQ") %>%
  dySeries("dV_et_R", color = "green", drawPoints = TRUE, pointShape = "triangle", pointSize = "3", label = "dV corrected by reservoir evaporation") %>% 
  dyGroup(c(paste0(c("dQ"), 1:R)), color = rep("pink", R)) 

```

